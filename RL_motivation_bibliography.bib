@article{li_datacenters,
	title = {Transforming {Cooling} {Optimization} for {Green} {Data} {Center} via {Deep} {Reinforcement} {Learning}},
	volume = {50},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2019.2927410},
	abstract = {Data center (DC) plays an important role to support services, such as e-commerce and cloud computing. The resulting energy consumption from this growing market has drawn significant attention, and noticeably almost half of the energy cost is used to cool the DC to a particular temperature. It is thus an critical operational challenge to curb the cooling energy cost without sacrificing the thermal safety of a DC. The existing solutions typically follow a two-step approach, in which the system is first modeled based on expert knowledge and, thus, the operational actions are determined with heuristics and/or best practices. These approaches are often hard to generalize and might result in suboptimal performances due to intrinsic model errors for large-scale systems. In this paper, we propose optimizing the DC cooling control via the emerging deep reinforcement learning (DRL) framework. Compared to the existing approaches, our solution lends itself an end-to-end cooling control algorithm (CCA) via an off-policy offline version of the deep deterministic policy gradient (DDPG) algorithm, in which an evaluation network is trained to predict the DC energy cost along with resulting cooling effects, and a policy network is trained to gauge optimized control settings. Moreover, we introduce a de-underestimation (DUE) validation mechanism for the critic network to reduce the potential underestimation of the risk caused by neural approximation. Our proposed algorithm is evaluated on an EnergyPlus simulation platform and on a real data trace collected from the National Super Computing Centre (NSCC) of Singapore. The resulting numerical results show that the proposed CCA can achieve up to 11\% cooling cost reduction on the simulation platform compared with a manually configured baseline control algorithm. In the trace-based study of conservative nature, the proposed algorithm can achieve about 15\% cooling energy savings on the NSCC data trace. Our pioneering approach can shed new light on the application of DRL to optimize and automate DC operations and management, potentially revolutionizing digital infrastructure management with intelligence.},
	number = {5},
	journal = {IEEE Transactions on Cybernetics},
	author = {Li, Yuanlong and Wen, Yonggang and Tao, Dacheng and Guan, Kyle},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Atmospheric modeling, Computational modeling, Cooling, Data center (DC) cooling optimization, Data models, deep learning, Mathematical model, Optimization, reinforcement learning (RL), Software algorithms},
	pages = {2002--2013},
	file = {Transforming_Cooling_Optimization_for_Green_Data_Center_via_Deep_Reinforcement_Learning.pdf:D\:\\Drive\\Etudes\\Ponts et Chaussees\\Stages\\2eme stage\\Travail\\References\\Contents\\Y.Li et al - Transforming_Cooling_Optimization_for_Green_Data_Center_via_Deep_Reinforcement_Learning.pdf:application/pdf},
}

@article{hwangbo_quadrotors,
	title = {Control of a {Quadrotor} with {Reinforcement} {Learning}},
	volume = {2},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/1707.05110},
	doi = {10.1109/LRA.2017.2720851},
	abstract = {In this paper, we present a method to control a quadrotor with a neural network trained using reinforcement learning techniques. With reinforcement learning, a common network can be trained to directly map state to actuator command making any predefined control structure obsolete for training. Moreover, we present a new learning algorithm which differs from the existing ones in certain aspects. Our algorithm is conservative but stable for complicated tasks. We found that it is more applicable to controlling a quadrotor than existing algorithms. We demonstrate the performance of the trained policy both in simulation and with a real quadrotor. Experiments show that our policy network can react to step response relatively accurately. With the same policy, we also demonstrate that we can stabilize the quadrotor in the air even under very harsh initialization (manually throwing it upside-down in the air with an initial velocity of 5 m/s). Computation time of evaluating the policy is only 7 \{{\textbackslash}mu\}s per time step which is two orders of magnitude less than common trajectory optimization algorithms with an approximated model.},
	number = {4},
	urldate = {2023-02-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Hwangbo, Jemin and Sa, Inkyu and Siegwart, Roland and Hutter, Marco},
	month = oct,
	year = {2017},
	note = {arXiv:1707.05110 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {2096--2103},
	file = {J.Hwangbo - Control of a Quadrotor with RL.pdf:D\:\\Drive\\Etudes\\Ponts et Chaussees\\Stages\\2eme stage\\Travail\\References\\Contents\\J.Hwangbo - Control of a Quadrotor with RL.pdf:application/pdf},
}

@article{geijtenbeek_bipedal_walk,
    author = {Geijtenbeek, Thomas and van de Panne, Michiel and van der Stappen, A. Frank},
    title = {Flexible Muscle-Based Locomotion for Bipedal Creatures},
    year = {2013},
    issue_date = {November 2013},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {32},
    number = {6},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/2508363.2508399},
    doi = {10.1145/2508363.2508399},
    abstract = {We present a muscle-based control method for simulated bipeds in which both the muscle routing and control parameters are optimized. This yields a generic locomotion control method that supports a variety of bipedal creatures. All actuation forces are the result of 3D simulated muscles, and a model of neural delay is included for all feedback paths. As a result, our controllers generate torque patterns that incorporate biomechanical constraints. The synthesized controllers find different gaits based on target speed, can cope with uneven terrain and external perturbations, and can steer to target directions.},
    journal = {ACM Trans. Graph.},
    month = {nov},
    articleno = {206},
    numpages = {11},
    keywords = {musculoskeletal simulation, physics-based animation}
}

@article{garnier_fluid_mechanics,
	title = {A review on deep reinforcement learning for fluid mechanics},
	volume = {225},
	issn = {0045-7930},
	url = {https://www.sciencedirect.com/science/article/pii/S0045793021001407},
	doi = {10.1016/j.compfluid.2021.104973},
	abstract = {Deep reinforcement learning (DRL) has recently been adopted in a wide range of physics and engineering domains for its ability to solve decision-making problems that were previously out of reach due to a combination of non-linearity and high dimensionality. In the last few years, it has spread in the field of computational mechanics, and particularly in fluid dynamics, with recent applications in flow control and shape optimization. In this work, we conduct a detailed review of existing DRL applications to fluid mechanics problems. In addition, we present recent results that further illustrate the potential of DRL in Fluid Mechanics. The coupling methods used in each case are covered, detailing their advantages and limitations. Our review also focuses on the comparison with classical methods for optimal control and optimization. Finally, several test cases are described that illustrate recent progress made in this field. The goal of this publication is to provide an understanding of DRL capabilities along with state-of-the-art applications in fluid dynamics to researchers wishing to address new problems with these methods.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Computers \& Fluids},
	author = {Garnier, Paul and Viquerat, Jonathan and Rabault, Jean and Larcher, Aurélien and Kuhnle, Alexander and Hachem, Elie},
	month = jul,
	year = {2021},
	keywords = {Deep reinforcement learning, Fluid mechanics},
	pages = {104973},
}

@article{novati_synchronisation_swimmers,
	title = {Synchronisation through learning for two self-propelled swimmers},
	volume = {12},
	issn = {1748-3190},
	url = {https://dx.doi.org/10.1088/1748-3190/aa6311},
	doi = {10.1088/1748-3190/aa6311},
	abstract = {The coordinated motion by multiple swimmers is a fundamental component in fish schooling. The flow field induced by the motion of each self-propelled swimmer implies non-linear hydrodynamic interactions among the members of a group. How do swimmers compensate for such hydrodynamic interactions in coordinated patterns? We provide an answer to this riddle though simulations of two, self-propelled, fish-like bodies that employ a learning algorithm to synchronise their swimming patterns. We distinguish between learned motion patterns and the commonly used a-priori specified movements, that are imposed on the swimmers without feedback from their hydrodynamic interactions. First, we demonstrate that two rigid bodies executing pre-specified motions, with an alternating leader and follower, can result in substantial drag-reduction and intermittent thrust generation. In turn, we study two self-propelled swimmers arranged in a leader-follower configuration, with a-priori specified body-deformations. These two self-propelled swimmers do not sustain their tandem configuration. The follower experiences either an increase or decrease in swimming speed, depending on the initial conditions, while the swimming of the leader remains largely unaffected. This indicates that a-priori specified patterns are not sufficient to sustain synchronised swimming. We then examine a tandem of swimmers where the leader has a steady gait and the follower learns to synchronize its motion, to overcome the forces induced by the leader’s vortex wake. The follower employs reinforcement learning to adapt its swimming-kinematics so as to minimize its lateral deviations from the leader’s path. Swimming in such a sustained synchronised tandem yields up to reduction in energy expenditure for the follower, in addition to a increase in its swimming-efficiency. The present results show that two self-propelled swimmers can be synchronised by adapting their motion patterns to compensate for flow-structure interactions. Moreover, swimmers can exploit the vortical structures of their flow field so that synchronised swimming is energetically beneficial.},
	language = {en},
	number = {3},
	urldate = {2023-03-23},
	journal = {Bioinspiration \& Biomimetics},
	author = {Novati, Guido and Verma, Siddhartha and Alexeev, Dmitry and Rossinelli, Diego and Rees, Wim M. van and Koumoutsakos, Petros},
	month = mar,
	year = {2017},
	note = {Publisher: IOP Publishing},
	pages = {036001},
	file = {G.Novati - synchronisation through learning for two self-propelled swimmers.pdf:D\:\\Drive\\Etudes\\Ponts et Chaussees\\Stages\\2eme stage\\Travail\\References\\Contents\\G.Novati - synchronisation through learning for two self-propelled swimmers.pdf:application/pdf},
}

@article{bachute_autonomous_driving,
	title = {Autonomous Driving Architectures: Insights of Machine Learning and Deep Learning Algorithms},
	journal = {Machine Learning with Applications},
	volume = {6},
	pages = {100164},
	year = {2021},
	issn = {2666-8270},
	doi = {https://doi.org/10.1016/j.mlwa.2021.100164},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827021000827},
	author = {Mrinal R. Bachute and Javed M. Subhedar},
	keywords = {Autonomous Driving, Localization, Motion planning, Pedestrian detection, Perception, Taxonomy},
	abstract = {Research in Autonomous Driving is taking momentum due to the inherent advantages of autonomous driving systems. The main advantage being the disassociation of the driver from the vehicle reducing the human intervention. However, the Autonomous Driving System involves many subsystems which need to be integrated as a whole system. Some of the tasks include Motion Planning, Vehicle Localization, Pedestrian Detection, Traffic Sign Detection, Road-marking Detection, Automated Parking, Vehicle Cybersecurity, and System Fault Diagnosis. This paper aims to the overview of various Machine Learning and Deep Learning Algorithms used in Autonomous Driving Architectures for different tasks like Motion Planning, Vehicle Localization, Pedestrian Detection, Traffic Sign Detection, Road-marking Detection, Automated Parking, Vehicle Cybersecurity and Fault Diagnosis. This paper surveys the technical aspects of Machine Learning and Deep Learning Algorithms used for Autonomous Driving Systems. Comparison of these algorithms is done based on the metrics like mean Intersect in over Union (mIoU), Average Precision (AP)missed detection rate, miss rate False Positives Per Image (FPPI), and average number for false frame detection. This study contributes to picture a review of the Machine Learning and Deep Learning Algorithms used for Autonomous Driving Systems and is organized based on the different tasks of the system.}
}


@article{berret_human_motion,
	title = {Why {Don}'t {We} {Move} {Slower}? {The} {Value} of {Time} in the {Neural} {Control} of {Action}},
	volume = {36},
	copyright = {Copyright © 2016 the authors 0270-6474/16/361056-15\$15.00/0},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Why {Don}'t {We} {Move} {Slower}?},
	url = {https://www.jneurosci.org/content/36/4/1056},
	doi = {10.1523/JNEUROSCI.1921-15.2016},
	abstract = {To want something now rather than later is a common attitude that reflects the brain's tendency to value the passage of time. Because the time taken to accomplish an action inevitably delays task achievement and reward acquisition, this idea was ported to neural movement control within the “cost of time” theory. This theory provides a normative framework to account for the underpinnings of movement time formation within the brain and the origin of a self-selected pace in human and animal motion. Then, how does the brain exactly value time in the control of action? To tackle this issue, we used an inverse optimal control approach and developed a general methodology allowing to squarely sample infinitesimal values of the time cost from experimental motion data. The cost of time underlying saccades was found to have a concave growth, thereby confirming previous results on hyperbolic reward discounting, yet without making any prior assumption about this hypothetical nature. For self-paced reaching, however, movement time was primarily valued according to a striking sigmoidal shape; its rate of change consistently presented a steep rise before a maximum was reached and a slower decay was observed. Theoretical properties of uniqueness and robustness of the inferred time cost were established for the class of problems under investigation, thus reinforcing the significance of the present findings. These results may offer a unique opportunity to uncover how the brain values the passage of time in healthy and pathological motor control and shed new light on the processes underlying action invigoration.
SIGNIFICANCE STATEMENT Movement time is a fundamental characteristic of neural motor control, but the principles underlying its formation remain little known. This work addresses that question within the inverse optimal control framework where the challenge is to uncover what optimality criterion underlies a system's behavior. Here we rely on the “cost of time” theory that finds its roots into the brain's tendency to discount the actual value of future reward. It asserts that the time elapsed until action completion entails a cost, thereby making slow moves nonoptimal. By means of a thorough theoretical analysis, the present article shows how to sample the infinitesimal values of the time cost without prior assumption about its hypothetical nature and emphasizes its sigmoidal shape for reaching.},
	language = {en},
	number = {4},
	urldate = {2023-02-08},
	journal = {Journal of Neuroscience},
	author = {Berret, Bastien and Jean, Frédéric},
	month = jan,
	year = {2016},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	keywords = {action vigor, cost of time, movement time, optimal control, reaching, saccade},
	pages = {1056--1070},
	annote = {An example of inverse optimal control problem: That of understanding why we make motions at the speed we do.
Here, the trajectories are the motions of e.g. the arm, and the speed of this motion (the movement time, MT) is assumed to be the optimal time minimizing the sum of the trajectory cost (basically, how hard it is to make an accurate motion) and of the cost of time. To find this cost of time (which is a function of time), they resolve an inverse problem consisting in finding this cost function given trajectories.
},
	file = {B.Berret and F.Jean - Why Don’t We Move Slower The Value of Time in the Neural.pdf:D\:\\Drive\\Etudes\\Ponts et Chaussees\\Stages\\2eme stage\\Travail\\References\\Contents\\B.Berret and F.Jean - Why Don’t We Move Slower The Value of Time in the Neural.pdf:application/pdf},
}

@article{sivak_quantum,
	title = {Model-{Free} {Quantum} {Control} with {Reinforcement} {Learning}},
	volume = {12},
	issn = {2160-3308},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.12.011059},
	doi = {10.1103/PhysRevX.12.011059},
	language = {en},
	number = {1},
	urldate = {2023-03-17},
	journal = {Physical Review X},
	author = {Sivak, V. V. and Eickbusch, A. and Liu, H. and Royer, B. and Tsioutsios, I. and Devoret, M. H.},
	month = mar,
	year = {2022},
	pages = {011059},
}

@article{rao_classical_numerical_control,
	title = {A {Survey} of {Numerical} {Methods} for {Optimal} {Control}},
	volume = {135},
	abstract = {A survey of numerical methods for optimal control is given. The objective of the article is to describe the major methods that have been developed over the years for solving general optimal control problems. In particular, the two broad classes of indirect and direct methods are discussed, the main approaches that are used in each class are described, and an extensive list is provided to relevant work in the open literature. Several important computational issues are then discussed and well known software programs for solving optimal control problems are described. Finally, a discussion is given on how to choose a method.},
	journal = {Advances in the Astronautical Sciences},
	author = {Rao, Anil},
	month = jan,
	year = {2010},
}

@article{biral_classical_numerical_control,
	title = {Notes on {Numerical} {Methods} for {Solving} {Optimal} {Control} {Problems}},
	volume = {5},
	issn = {2187-1094, 2187-1108},
	url = {https://www.jstage.jst.go.jp/article/ieejjia/5/2/5_154/_article},
	doi = {10.1541/ieejjia.5.154},
	abstract = {Recent advances in theory, algorithms, and computational power make it possible to solve complex, optimal control problems both for oﬀ-line and on-line industrial applications. This paper starts by reviewing the technical details of the solution methods pertaining to three general categories: dynamic programming, indirect methods, and direct methods. With the aid of a demonstration example, the advantages and disadvantages of each method are discussed, along with a brief review of available software. The main result that emerges is the indirect method being numerically competitive with the performance of direct ones based on non-linear programming solvers and interior point algorithms. The second part of the paper introduces an indirect method based on the Pontryagin Minimum Principle (PMP). It also presents a detailed procedure and software tools (named PINS) to formulate the problem, automatically generate the C++ code, and eventually obtain a numerical solution for several optimal control problems of practical relevance. The application of PMP relates to the analytical derivation of necessary conditions for optimality. This aspect—often regarded in the literature as a drawback—is here exploited to build a robust yet eﬃcient numerical method that formally eliminates the controls from the resulting Boundary Value Problem, thus gaining robustness and a high convergence rate. The elimination of the control is obtained either via their explicit formulation function of state and Lagrange multipliers—when possible—or via an iterative numerical solution. The paper closes presenting a minimum time manoeuvre of a car using a fairly complex vehicle model which also includes tyre saturation.},
	language = {en},
	number = {2},
	urldate = {2023-03-28},
	journal = {IEEJ Journal of Industry Applications},
	author = {Biral, Francesco and Bertolazzi, Enrico and Bosetti, Paolo},
	year = {2016},
	pages = {154--166},
	file = {Biral et al. - 2016 - Notes on Numerical Methods for Solving Optimal Con.pdf:C\:\\Users\\Theilo\\Zotero\\storage\\DHVW2SBM\\Biral et al. - 2016 - Notes on Numerical Methods for Solving Optimal Con.pdf:application/pdf},
}

@article{allahverdi_numerical_control_burgers,
	title = {Numerical aspects of large-time optimal control of {Burgers} equation},
	volume = {50},
	copyright = {© EDP Sciences, SMAI 2016},
	issn = {0764-583X, 1290-3841},
	url = {https://www.esaim-m2an.org/articles/m2an/abs/2016/05/m2an150015/m2an150015.html},
	doi = {10.1051/m2an/2015076},
	abstract = {In this paper, we discuss the efficiency of various numerical methods for the inverse design of the Burgers equation, both in the viscous and in the inviscid case, in long time-horizons. Roughly, the problem consists in, given a final desired target, to identify the initial datum that leads to it along the Burgers dynamics. This constitutes an ill-posed backward problem. We highlight the importance of employing a proper discretization scheme in the numerical approximation of the equation under consideration to obtain an accurate approximation of the optimal control problem. Convergence in the classical sense of numerical analysis does not suffice since numerical schemes can alter the dynamics of the underlying continuous system in long time intervals. As we shall see, this may end up affecting the efficiency on the numerical approximation of the inverse design, that could be polluted by spurious high frequency numerical oscillations. To illustrate this, two well-known numerical schemes are employed: the modified Lax−Friedrichs scheme (MLF) and the Engquist−Osher (EO) one. It is by now well-known that the MLF scheme, as time tends to infinity, leads to asymptotic profiles with an excess of viscosity, while EO captures the correct asymptotic dynamics. We solve the inverse design problem by means of a gradient descent method and show that EO performs robustly, reaching efficiently a good approximation of the minimizer, while MLF shows a very strong sensitivity to the selection of cell and time-step sizes, due to excess of numerical viscosity. The achieved numerical results are confirmed by numerical experiments run with the open source nonlinear optimization package (IPOPT).},
	language = {en},
	number = {5},
	urldate = {2023-03-29},
	journal = {ESAIM: Mathematical Modelling and Numerical Analysis},
	author = {Allahverdi, Navid and Pozo, Alejandro and Zuazua, Enrique},
	month = sep,
	year = {2016},
	note = {Number: 5
Publisher: EDP Sciences},
	pages = {1371--1401},
}
